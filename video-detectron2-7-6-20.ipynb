{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import key libraries\n",
    "from PIL import Image, ImageDraw\n",
    "import torch, torchvision\n",
    "import detectron2\n",
    "from time import time\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "import scipy.ndimage\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# import our utilities\n",
    "import utils.filters as filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_mp4_frames(link):\n",
    "    # create video capture instance\n",
    "    vidcap = cv2.VideoCapture(link)\n",
    "    success,image = vidcap.read()\n",
    "    frames = [image]\n",
    "    # iterate over video frames, save each in a list\n",
    "    while success:\n",
    "        success,image = vidcap.read()\n",
    "        frames.append(image)\n",
    "    return frames[:-1]\n",
    "\n",
    "def create_predictor():\n",
    "    # create Detectron2 config and Default Predictor to run image inference\n",
    "    cfg = get_cfg()\n",
    "    # add project-specific config here if not running a model in Detectron2's core library\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .4\n",
    "    cfg.MODEL.DEVICE = 'cpu'\n",
    "    # find a model from detectron2's model zoo.\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    # build predictor\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    return predictor\n",
    "\n",
    "def blur_people(img, predictor):\n",
    "    # image in tensor form\n",
    "    im = np.array(img)\n",
    "    # image dimensions\n",
    "    height, width, channels = im.shape\n",
    "    # run inference on the image\n",
    "    outputs = predictor(im)\n",
    "    # get indices of predicted instances that are labelled as people\n",
    "    person_idx = (outputs[\"instances\"].pred_classes==0)\n",
    "    # count of 'people' instances\n",
    "    person_count = np.sum(np.array(person_idx))\n",
    "    # get image masks corresponding to each identified person\n",
    "    person_masks_tensor = outputs[\"instances\"].pred_masks[person_idx,:,:]\n",
    "    # create PIL image masks\n",
    "    person_masks = Image.new('L', im.shape[:-1], 0)\n",
    "    draw = ImageDraw.Draw(person_masks)    \n",
    "    # create mask and add peoples' shape to it\n",
    "    sharp_mask = np.zeros((height, width, channels))\n",
    "    for i in range(person_count):\n",
    "        sharp_mask[person_masks_tensor[i,:,:]==1]=1\n",
    "    # filtered image\n",
    "    result = (np.multiply(filter_image(img),sharp_mask)+img).astype(dtype=np.uint8)            \n",
    "    return result\n",
    "\n",
    "def pixelate_people(img, predictor):\n",
    "    img = Image.fromarray(img)\n",
    "    im = np.array(img)\n",
    "    height, width, channels = im.shape\n",
    "    outputs = predictor(im)\n",
    "    \n",
    "    person_idx = (outputs[\"instances\"].pred_classes == 0)\n",
    "    person_count = np.sum(np.array(person_idx))\n",
    "    person_masks_tensor = outputs[\"instances\"].pred_masks[person_idx, :, :]\n",
    "    \n",
    "    person_masks = Image.new('L', im.shape[:-1], 0)\n",
    "    draw = ImageDraw.Draw(person_masks)\n",
    "    \n",
    "    sharp_mask = np.zeros((height, width, channels))\n",
    "    for i in range(person_count):\n",
    "        sharp_mask[person_masks_tensor[i,:,:]==1]=1\n",
    "    \n",
    "    mask = sharp_mask.astype(np.bool)\n",
    "    result = (filters.contour(img) * mask + img * ~mask).astype(dtype=np.uint8)\n",
    "        \n",
    "    return result\n",
    "                          \n",
    "def filter_image(sharp_image):\n",
    "    # does what it says on the tin\n",
    "    blurred_image = scipy.ndimage.filters.sobel(sharp_image, mode='constant')\n",
    "    blurred_image = scipy.ndimage.filters.prewitt(blurred_image, mode='reflect')\n",
    "    return blurred_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'video/choreoshort.mp4'\n",
    "frames = collect_mp4_frames(link)\n",
    "\n",
    "# use default detectron2 predictor\n",
    "predictor = create_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "13.245033112582782% done\n",
      "26.490066225165563% done\n",
      "39.735099337748345% done\n",
      "52.980132450331126% done\n",
      "66.22516556291392% done\n",
      "79.47019867549669% done\n",
      "92.71523178807946% done\n"
     ]
    }
   ],
   "source": [
    "blurred_frames = []\n",
    "for i, frame in enumerate(frames):\n",
    "    if i % 100 == 0: \n",
    "        print ( f'{i/len(frames)*100}% done' )\n",
    "    blurred_frames.append(pixelate_people(frame, predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "folder_save = 'video/contour_choreo'\n",
    "video_name = 'contour_choreo4.avi'\n",
    "\n",
    "for i, frame in enumerate(blurred_frames):\n",
    "    Image.fromarray(frame).save(f'{folder_save}/{i}.png')\n",
    "    if i % 100 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "images = [img for img in os.listdir(folder_save) if img.endswith(\".png\")]\n",
    "images.sort()\n",
    "frame = cv2.imread(os.path.join(folder_save, images[0]))\n",
    "print(frame.shape)\n",
    "\n",
    "height, width, layers = (720,1280,3)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "video = cv2.VideoWriter(video_name, fourcc, 30, (width,height))\n",
    "\n",
    "for i in range(len(frames)):\n",
    "    img_name =  f'{i}.png'\n",
    "    video.write(cv2.imread(os.path.join(folder_save, img_name)))\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
